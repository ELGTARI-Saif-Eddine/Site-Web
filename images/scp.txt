# 1st step install and import modules

import requests
from bs4 import BeautifulSoup
import csv
from itertools import zip_longest
import pandas as pd

job_titles = []
company_names = []
locations_names = []

# 2nd step use requests to fetch the url
# fetch the link of the COVID-19 and fetch in page details
result = requests.get("URL")

#3rd step save page content/markup 
src = result.content
#print(src)

# 4th step create soup object to parse content
soup = BeautifulSoup(src, "lxml")
#print(soup)

# 5th step find the elements containing info we need
#-- geo_type, region, transportation_type, alternative_name
#-- sub-region, country, date, value
job_titles = soup.find_all("h2",{"class":"css-m6044qf"})
company_names = soup.find_all("h2",{"class":"css-m6044qf"})
locations_names = soup.find_all("h2",{"class":"css-m6044qf"})

#6th step loopover returned lists to extract needed info intolists
for i in range(len(job_titles)):
         job_title.append(job_titles[i].text)
		 company_name.append(company_names[i].text)
         locations_name.append(locations_names[i].text)

#7th step create csv file and fillit with values
file_list = [job_title, company_name, locations_name]
exported = zip_longest(*file_list)

with open("covid_collected_data.csv", "w") as myfile:
    wr = csv.writer(myfile)
    wr.writerow(["job title","company name","location"])
    wr.writerows(exported)	



		 