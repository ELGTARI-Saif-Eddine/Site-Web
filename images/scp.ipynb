{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNext_test_technique.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise No:1 - Apple mobility data**"
      ],
      "metadata": {
        "id": "LL0siZ5NgpB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st step import libraries\n",
        "from pathlib import Path # To check files in colab folders\n",
        "import pandas as pd  # dataFrames using pandas\n"
      ],
      "metadata": {
        "id": "eXu0d6bUc4ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNTWOEOQVpen"
      },
      "outputs": [],
      "source": [
        "# 2en step fetch the url\n",
        "# fetch the link of the COVID-19 and download \"CSV\" file\n",
        "!wget https://covid19-static.cdn-apple.com/covid19-mobility-data/2210HotfixDev17/v3/en-us/applemobilitytrends-2022-03-27.csv\n",
        "# CSV file will be downloaded and saved in the main folder in GOOGLE-COLAB repository\n",
        "\n",
        "\n",
        "\n",
        "### Check if \"applemobilitytrends\" file is downloaded successfully\n",
        "my_file = Path(\"applemobilitytrends-2022-03-27.csv\")\n",
        "if my_file.is_file():\n",
        "  print(\"File Downloaded Successfully.\")\n",
        "else:\n",
        "  print(\"Error Downloading File! Please check the URL or try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd step   \n",
        "#Create a function that take no argument and return a dataframe with the following columns :\n",
        "#-- geo_type, region, transportation_type ,alternative_name, sub-region, country, date, value\n",
        "\n",
        "def getDataframe():\n",
        "  df = pd.read_csv('applemobilitytrends-2022-03-27.csv')\n",
        "  return df\n",
        "\n",
        "print(getDataframe().head(10)) #-- Visualization: show 10 rows from dataFrame\n"
      ],
      "metadata": {
        "id": "-MGyN1EgWYOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise No:2 - FSA data**"
      ],
      "metadata": {
        "id": "r6n--Tr4g6Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##################################### \n",
        "# In the following exercise, I used the \"BeautifulSoup\" library because it is very simple, very powerful, runs quickly, \n",
        "# and allows us to extract data from a badly written web page. It structures an HTML or XML web page, gets the data you\n",
        "# need from amongst the lot, and allows you to extract the required data in the right format.    \n",
        "#####################################\n",
        "\n",
        "\n",
        "# 1st step install and import modules\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from itertools import zip_longest\n",
        "import pandas as pd\n",
        "\n",
        "years_list = []\n",
        "\n",
        "# 2nd step use requests to fetch the url\n",
        "# fetch the link of the FSA Crop Acreage Data Reported to FSA and fetch in page details\n",
        "url = \"https://www.fsa.usda.gov/news-room/efoia/electronic-reading-room/frequently-requested-information/crop-acreage-data/index\"\n",
        "result = requests.get(url)\n",
        "\n",
        "#3rd step save page content/markup \n",
        "src = result.content\n",
        "#print(src)\n",
        "\n",
        "# 4th step create soup object to parse content\n",
        "soup = BeautifulSoup(src, \"lxml\")\n",
        "#print(soup)\n",
        "\n",
        "# 5th step find the elements containing info we need\n",
        "#-- years, \n",
        "years = soup.find_all(\"div\",{\"class\":\"rxbodyfield\"})\n",
        "\n",
        "for i in range(len(years)):\n",
        "  sp = BeautifulSoup(str(years[i]), \"html.parser\")\n",
        "  year = sp.find_all(\"h3\")\n",
        "\n",
        "# a simple fucntion to get digits from a string\n",
        "def only_numerics(seq):\n",
        "    seq_type= type(seq)\n",
        "    return seq_type().join(filter(seq_type.isdigit, seq))\n",
        "    \n",
        "#6th step loopover returned lists to extract needed info intolists\n",
        "for i in range(len(year)-1):\n",
        "           years_list.append(only_numerics(year[i].text))\n",
        "\n",
        "\n",
        "# Final result List of years:\n",
        "print(\"List Of Years:\")\n",
        "print(years_list)\n",
        "print(\"  \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Extract \"country_data\" sheet, \"country_data_links\" and  \"publication_date\" function:\n",
        "def extractDataFromWebsite(list_yr=[]): # default parameters: empty list  \n",
        "\n",
        "  # Start Collecting DATA by year\n",
        "  for i in range(len(years)):\n",
        "    sp = BeautifulSoup(str(years[i]), \"html.parser\")\n",
        "    data_conut = sp.find_all(\"li\")\n",
        "\n",
        "\n",
        "  data_link = []\n",
        "  data_country =[]\n",
        "  publication_date =[]\n",
        "  ls_years =[]\n",
        "\n",
        "  for i in range(len(data_conut)-1):\n",
        "    try: \n",
        "      data_link.append('https://www.fsa.usda.gov' + data_conut[i].find(\"a\").attrs['href'])\n",
        "      data_country.append(data_conut[i].find(\"a\").text) #.attrs['title'])\n",
        "\n",
        "      strg = data_conut[i].find(\"a\").text\n",
        "      find_of = strg.find(' of ') + 4\n",
        "      length_str = len(strg)\n",
        "      if(strg.find(' of ') == -1):\n",
        "        publication_date.append('None')\n",
        "      else:\n",
        "        publication_date.append(strg[find_of:length_str])\n",
        "\n",
        "\n",
        "    except Exception:\n",
        "      data_country.append(data_conut[i].text)\n",
        "      data_link.append('None')\n",
        "      publication_date.append('None')\n",
        "      pass\n",
        "\n",
        " \n",
        "\n",
        "  #create csv file andfill it with values\n",
        "  file_list = [data_country, data_link, publication_date]\n",
        "  exported = zip_longest(*file_list)\n",
        "\n",
        "  with open(\"fsa_crop_acreage_data.csv\", \"w\") as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow([\"Data Country\",\"Data Link\",\"Publication Date\"])\n",
        "      wr.writerows(exported)\n",
        "\n",
        "  # CSV file created successfully\n",
        "  print(\"CSV file created successfully...\")\n",
        "  print(\"Check default folder or wait for few seconds to see < fsa_crop_acreage_data.csv > File.\")\n",
        "  \n",
        "  # Added Data dataframe\n",
        "  df = pd.read_csv('fsa_crop_acreage_data.csv')\n",
        "\n",
        "  # return dataframe as final result\n",
        "  return df\n",
        "\n",
        "# test extractDataFromWebsite() function\n",
        "dataFrame = extractDataFromWebsite(years_list)\n",
        "print(\"\")\n",
        "print(dataFrame)\n"
      ],
      "metadata": {
        "id": "33cfCoPbheB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}